{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f440b0",
   "metadata": {},
   "source": [
    "# EDA for Credit Card Dataset\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "1. Install Dependencies\n",
    "2. Imports\n",
    "3. Load Data\n",
    "4. Data Inspection\n",
    "5. Data Cleaning\n",
    "6. Descriptive Statistics\n",
    "7. Correlation Analysis\n",
    "8. Correlation Heatmap\n",
    "9. Collinearity Analysis (VIF)\n",
    "10. Feature-Target Separation and Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341225b",
   "metadata": {},
   "source": [
    "## 1) Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e927698",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4a771",
   "metadata": {},
   "source": [
    "## 2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06897f65",
   "metadata": {},
   "source": [
    "## 3) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71315b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/UCI_Credit_card.csv')\n",
    "\n",
    "# Initial inspection\n",
    "print('Data shape:', df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f04dbc",
   "metadata": {},
   "source": [
    "## 4) Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b63488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# Missing values\n",
    "print('\\nMissing values:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Duplicate rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f'\\nNumber of duplicate rows: {num_duplicates}')\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ce2dd",
   "metadata": {},
   "source": [
    "## 5) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=['ID'])\n",
    "\n",
    "# Recode EDUCATION: 0, 5, 6 -> 4\n",
    "df['EDUCATION'] = df['EDUCATION'].replace({0: 4, 5: 4, 6: 4})\n",
    "\n",
    "# Recode MARRIAGE: 0 -> 3\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace({0: 3})\n",
    "\n",
    "# Inspect cleaned data\n",
    "print('After cleaning, shape:', df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172a17c",
   "metadata": {},
   "source": [
    "## 6) Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9bc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f70ca3",
   "metadata": {},
   "source": [
    "## 7) Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "print('Correlation Matrix:') \n",
    "print(corr_matrix)\n",
    "\n",
    "# Identify highly correlated pairs (|corr| > 0.8)\n",
    "threshold = 0.8\n",
    "high_corr_pairs = [\n",
    "    (numeric_cols[i], numeric_cols[j], corr_matrix.iloc[i, j])\n",
    "    for i in range(len(numeric_cols))\n",
    "    for j in range(i + 1, len(numeric_cols))\n",
    "    if abs(corr_matrix.iloc[i, j]) > threshold\n",
    "]\n",
    "if high_corr_pairs:\n",
    "    print(f'\\nHighly correlated pairs (|corr| > {threshold}):')\n",
    "    for var1, var2, val in high_corr_pairs:\n",
    "        print(f'{var1} - {var2}: {val:.2f}')\n",
    "else:\n",
    "    print(f'\\nNo pairs with |corr| > {threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdab8d",
   "metadata": {},
   "source": [
    "## 8) Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False, fmt='.2f',\n",
    "            xticklabels=numeric_cols, yticklabels=numeric_cols)\n",
    "plt.title('Correlation Matrix Heatmap', fontsize=16)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a053ac",
   "metadata": {},
   "source": [
    "## 9) Collinearity Analysis (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Variance Inflation Factor (VIF)\n",
    "X = df[numeric_cols].dropna().copy()\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "vif_data['VIF'] = [\n",
    "    variance_inflation_factor(X.values, i) \n",
    "    for i in range(X.shape[1])\n",
    "]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575be345",
   "metadata": {},
   "source": [
    "## 10) Feature-Target Separation and Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'default.payment.next.month'\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Split 60% train, 20% val, 20% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    train_size=0.6, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    stratify=y_temp, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display shapes\n",
    "print(f'Training set: X={X_train.shape}, y={y_train.shape}')\n",
    "print(f'Validation set: X={X_val.shape}, y={y_val.shape}')\n",
    "print(f'Test set: X={X_test.shape}, y={y_test.shape}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
